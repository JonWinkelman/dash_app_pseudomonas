{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66c38537-44e8-4a75-9148-be6a31eda33b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncbi.datasets module not found. To install, run `pip install ncbi-datasets-pylib`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from jw_utils import ncbi_datasets_fxs as ndf\n",
    "from orthofinder_utils import dash_ortho_parser_d as dop\n",
    "from orthofinder_utils import dash_app_preprocess as dap\n",
    "from orthofinder_utils import proteomes_for_orthofinder as pfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb72b5f2-59ff-41ac-87a8-b643d545e1da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d62abff3-7c44-47b5-be89-cda15304eb66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder = './ncbi_dataset/data'\n",
    "ndf.check_for_file(data_folder, prefix = 'GC', suffix = '.gff')\n",
    "ndf.check_for_file(data_folder, prefix = 'GC', suffix = '.faa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5f1e217-36f1-4509-875f-e783501cc7c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accessions = ndf.copy_proteomes_to_own_folder(data_folder)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8957b09-5fce-425c-8c34-da6865fa1d69",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "848c9bc3-0d0c-47ae-9917-d5dcfde0a5fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Copy ./Proteomes to aws s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88f6c34e-e200-4452-95b0-e1c108890ac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "proteomes_folder = './Proteomes/'aw\n",
    "s3_dest_folder = 's3://mukherjee-lab/dash_app_pseudomonas/Proteomes/'\n",
    "!aws s3 cp $proteomes_folder $s3_dest_folder --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3412e0a1-c794-4595-b722-5d371bbb5f09",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Running OrthoFinder on AWS involves several steps. Here is a general outline of the process:  \n",
    "go to aws console and sign in: https://aws.amazon.com/console/\n",
    "1. Launch an EC2 instance on AWS. https://us-east-2.console.aws.amazon.com/ec2/home?region=us-east-2#Home:\n",
    "    - I launch Amazon Linux Amazon Machine Image (AMI)  \n",
    "    - Choose instance type with appropriate number of cores and memory\n",
    "2. Configure the instance with the necessary software and dependencies, such as Python and  \n",
    "OrthoFinder.  \n",
    "3. Transfer your input data to the instance.  \n",
    "4. Run OrthoFinder on the instance.  \n",
    "5. Transfer the output data back to your local machine.  \n",
    "#### Here are more detailed instructions:  \n",
    "1. Launch an EC2 instance on AWS:  \n",
    "    1. Log in to your AWS account.  \n",
    "    2. Navigate to the EC2 dashboard.  \n",
    "    3. Click \"Launch Instance\" to launch a new instance.  \n",
    "    4. Choose an appropriate Amazon Machine Image (AMI) for your instance. You can use a pre-configured AMI that already has   \n",
    "OrthoFinder installed or you can create a custom AMI with OrthoFinder installed.  \n",
    "        1. e.g. Amazon Linux 2023 AMI  \n",
    "    5. Select an instance type that meets your requirements. OrthoFinder is not very resource-intensive,  \n",
    "so a small or medium-sized instance should be sufficient for most analyses   \n",
    "        1. T2 micro is free or very cheap  \n",
    "    6. Configure any additional settings as necessary (e.g., security groups, key pairs, etc.).\n",
    "        1. Select or create a key-pair name e.g. jon-orthofinder\n",
    "    7. Launch the instance.  \n",
    "        1. There are several ‘connect’ buttons to push here...  \n",
    "2. Configure the EC2 instance:  \n",
    "    1. Once your instance is running, connect to it using SSH  \n",
    "    2. configure aws in the terminal\n",
    "        - `$ aws configure`\n",
    "        - `Enter access key: $ ######`\n",
    "        - `Secret key: $ #####`  \n",
    "        - `Region name: $ us-east-2`  \n",
    "        - `Default output format: $ json`  \n",
    "    3. Install any necessary dependencies, such as Python and OrthoFinder.\n",
    "        - `$ mkdir ~/orthofinder`  \n",
    "        - `$ cd ./orthofinder`\n",
    "        - `$ wget https://github.com/davidemms/OrthoFinder/releases/latest/download/OrthoFinder.tar.gz`  \n",
    "        - `$ tar xzvf OrthoFinder.tar.gz`  \n",
    "        - `$ cd OrthoFinder/`  \n",
    "        - Check to see that orthofinder is installed and working.\n",
    "        - `$ ./orthofinder -h`  \n",
    "    4. You may need to configure your environment variables to point to the correct paths for OrthoFinder and other software  \n",
    "3. Transfer your input data to the instance:  \n",
    "    - There are several ways to transfer data to your instance, including SCP, SFTP, or AWS S3.  \n",
    "    - For orthofinder, I transfered data from an aws s3 bucketL:\n",
    "        1. `$ mkdir ./Proteomes`  \n",
    "        2. `$ aws s3 cp s3://mukherjee-lab/dash_app_bdelivibrio/Proteomes ./Proteomes --recursive`\n",
    "4. Run OrthoFinder:  \n",
    "    - Run OrthoFinder using the command-line interface, specifying the appropriate input and output directories.\n",
    "    - `$ ./orthofinder -f ./Proteomes`  \n",
    "    - Monitor the progress of the analysis and ensure that it completes successfully.\n",
    "5. Transfer the output data back to your local machine:\n",
    "   Once OrthoFinder has finished running, transfer the output data back to your local machine using the same method used to transfer the input data.\n",
    "\n",
    "***Note that these steps are a general outline and may need to be modified based on your specific analysis requirements and the resources available to you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb0f49b-b04a-423d-94aa-8ac7d9a175d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Script for running orthofinder on aws          \n",
    "\n",
    "        #!/bin/bash  \n",
    "        ##### Set AWS credentials  \n",
    "        export AWS_ACCESS_KEY_ID=#####  \n",
    "        export AWS_SECRET_ACCESS_KEY=####\n",
    "        export DEFAULT_REGION_NAME=us-east-2  \n",
    "        export DEFAULT_OUTPUT_FORMAT=json  \n",
    "\n",
    "        ##### Install OrthoFinder  \n",
    "        mkdir ~/orthofinder  \n",
    "        cd ./orthofinder  \n",
    "        wget https://github.com/davidemms/OrthoFinder/releases/latest/download/OrthoFinder.tar.gz   \n",
    "        tar xzvf OrthoFinder.tar.gz   \n",
    "        cd OrthoFinder/   \n",
    "        mkdir ./Proteomes    \n",
    "\n",
    "        ##### Download files from S3 bucket  \n",
    "        aws s3 cp s3://mukherjee-lab/dash_app_bdelivibrio/Proteomes ./Proteomes --recursive   \n",
    "\n",
    "        ##### Confirm files were downloaded  \n",
    "        ls ./Proteomes  \n",
    "        current_date=$(date +%Y-%m-%d)  \n",
    "        output_folder=${current_date}_OF_Results  \n",
    "        ./orthofinder -f ./Proteomes --output $output_folder  \n",
    "\n",
    "        ##### install pigz to compress directory using multiple threads and move compressed results to s3 bucket on aws  \n",
    "        sudo yum install pigz -y  \n",
    "        output_folder_compressed=${output_folder}.tar.gz  \n",
    "        time tar -I pigz -cf $output_folder_compressed $output_folder  \n",
    "        echo $output_folder  \n",
    "        aws s3 cp $output_folder_compressed s3://mukherjee-lab/dash_app_bdelivibrio/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baaf2687-125f-438f-bfc1-dd43361aba7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e4ae78-30d6-4451-834d-dcc83acc5204",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7617e80-3638-4777-8a1e-b9655a25406b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
